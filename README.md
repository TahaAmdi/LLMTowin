# üß† LLMTowin  
> Retrieval-Augmented LLM System for Intelligent Content Crawling & Semantic Search

LLMTowin is a modular LLM engineering framework designed to crawl, embed, and semantically retrieve user-generated content from multiple sources (LinkedIn, GitHub, Medium, etc.) using a Retrieval-Augmented Generation (RAG) pipeline.  

It follows the architectural patterns described in *The LLM Engineers Handbook*, combining:
- MongoDB for NoSQL persistence  
- Qdrant for vector similarity search  
- SentenceTransformers for embeddings  
- FastAPI and Poetry for deployment and environment management  

---

## üöÄ Features

### üï∑Ô∏è Crawlers  
Located in llm_engineering/application/crawlers/  

Crawlers extract public data from multiple platforms:
- linkedin.py ‚Üí posts, comments, and profiles  
- medium.py ‚Üí articles and topics  
- github.py ‚Üí repositories and project metadata  

A central dispatcher.py orchestrates these crawlers and passes the data to the ETL pipeline.

---

### üß© Networks  
Located in llm_engineering/application/networks/

- embeddings.py ‚Üí loads a SentenceTransformer model (default: `all-MiniLM-L6-v2`) and generates embeddings.  
- base.py ‚Üí defines a SingletonMeta class to prevent redundant model loading across the system.  

Supported models:
- SentenceTransformers  
- CrossEncoders  
- HuggingFace AutoTokenizer  

---

### üß† Domain Models  
Located in llm_engineering/domain/

Defines core data abstractions and shared types:
- documents.py ‚Üí Base document structure for MongoDB  
- types.py ‚Üí Enum (`DataCategory`) for dataset tags like posts, articles, repositories  
- exceptions.py ‚Üí Custom exception handling and structured logging  

---

### üß± Infrastructure  
Located in llm_engineering/infrastructure/db/

- mongo.py ‚Üí manages database connections and persistence operations  
- qdrant.py ‚Üí handles vector DB interactions and similarity searches  

---

### ‚öôÔ∏è Pipelines  
Located in pipelines/ and steps/etl/

Implements the ETL (Extract‚ÄìTransform‚ÄìLoad) logic:  
- digital_data_etl.py ‚Üí orchestrates the end-to-end data ingestion & embedding pipeline  
- crawl_links.py ‚Üí manages crawl queue and scheduling  
- get_or_create_user.py ‚Üí ensures user data is created or fetched across MongoDB & Qdrant  

---

## üß¨ RAG Pipeline Overview

The RAG system connects three main components:

1. Ingestion Pipeline ‚Üí crawls and embeds external data into Qdrant  
2. Retrieval Pipeline ‚Üí queries Qdrant for semantically relevant vectors  
3. Generation Pipeline ‚Üí injects retrieved data into the LLM prompt to generate contextual responses  

---

### üîÑ Example Flow
Example:  
> ‚ÄúSummarize my most engaging GitHub projects based on semantic similarity.‚Äù

The system retrieves your repositories, embeds descriptions, and returns summarized insights using the RAG pipeline.

---

## üß© Tech Stack

| Component | Library |
|------------|----------|
| Framework | FastAPI, Poetry |
| Data Storage | MongoDB |
| Vector DB | Qdrant |
| Embeddings | SentenceTransformers, CrossEncoder |
| Tokenization | HuggingFace Transformers |
| Pipeline Orchestration | ZenML |
| Logging | Loguru |
| ETL | BeautifulSoup, TQDM, Requests |

---

## ‚ö° Setup & Run

```bash
# 1Ô∏è‚É£ Install dependencies
poetry install

# 2Ô∏è‚É£ Run the digital ETL pipeline
poetry run python pipelines/digital_data_etl.py

# 3Ô∏è‚É£ Generate sentence embeddings
poetry run python llm_engineering/application/networks/embeddings.py
